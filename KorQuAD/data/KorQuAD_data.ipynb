{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KorQuAD_data",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG7xP3iigloK",
        "outputId": "cca34640-f2d2-4d4f-fb7e-b362f624cd32"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 70.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0hD4XoWgqyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca5c2ae7-3fbb-4d85-f35c-d82e3a8e826e"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "from tensorflow.keras.utils import get_file\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# 학습용 \n",
        "train_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\"\n",
        "train_path = get_file(\"train.json\", train_data_url) \n",
        "\n",
        "# 평가용 \n",
        "eval_data_url = \"https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\"\n",
        "eval_path = get_file(\"eval.json\", eval_data_url)\n",
        "\n",
        "train_data = json.load(open(train_path)) \n",
        "dev_data = json.load(open(eval_path))\n",
        "\n",
        "print(train_path)\n",
        "print(eval_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://korquad.github.io/dataset/KorQuAD_v1.0_train.json\n",
            "38535168/38527475 [==============================] - 0s 0us/step\n",
            "38543360/38527475 [==============================] - 0s 0us/step\n",
            "Downloading data from https://korquad.github.io/dataset/KorQuAD_v1.0_dev.json\n",
            "3883008/3881058 [==============================] - 0s 0us/step\n",
            "3891200/3881058 [==============================] - 0s 0us/step\n",
            "/root/.keras/datasets/train.json\n",
            "/root/.keras/datasets/eval.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5-0GfSUsyOZ"
      },
      "source": [
        "MAX_SEQ_LEN = 128\n",
        "MAX_TRAIN_LEN = 50000  # 시간이 오래 걸려서 데이터 개수를 제한한다.\n",
        "MAX_TEST_LEN = 1000\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', cache_dir='bert_ckpt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtYYkDHszlz2"
      },
      "source": [
        "def parsing(p_data, max_len = 50000):\n",
        "    context = []\n",
        "    question = []\n",
        "    start_idx = []\n",
        "    end_idx = []\n",
        "    for item in p_data[\"data\"]:\n",
        "        for para in item[\"paragraphs\"]:\n",
        "            for qa in para[\"qas\"]:\n",
        "                i_start = qa[\"answers\"][0][\"answer_start\"]\n",
        "                s_answer = qa[\"answers\"][0][\"text\"]\n",
        "                i_end = i_start + len(s_answer)\n",
        "                quest = qa[\"question\"]\n",
        "                \n",
        "                if i_end < MAX_SEQ_LEN - len(quest):\n",
        "                    context.append(para[\"context\"])\n",
        "                    question.append(quest)\n",
        "                    start_idx.append(i_start)\n",
        "                    end_idx.append(i_end)\n",
        "    \n",
        "    # question과 paragraph으로 BERT의 입력 데이터를 생성한다.\n",
        "    qa_pairs = list(zip(question, context))\n",
        "    qa_enc = tokenizer.batch_encode_plus(\n",
        "                qa_pairs,\n",
        "                add_special_tokens = True,\n",
        "                padding = True,\n",
        "                truncation = True, \n",
        "                max_length = MAX_SEQ_LEN,\n",
        "                return_attention_mask = True,\n",
        "                return_token_type_ids=True,\n",
        "                return_tensors = 'tf')        \n",
        "\n",
        "    x_ids = qa_enc['input_ids'].numpy()\n",
        "    x_msk = qa_enc['attention_mask'].numpy()\n",
        "    x_typ = qa_enc['token_type_ids'].numpy()\n",
        "    \n",
        "    # KorQuAD 모델의 최종 출력 target\n",
        "    y_start = np.array(start_idx)\n",
        "    y_end = np.array(end_idx)\n",
        "        \n",
        "    return x_ids, x_msk, x_typ, y_start, y_end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_W3lxPMzuUM"
      },
      "source": [
        "x_train_ids, x_train_msk, x_train_typ, y_train_start, y_train_end = parsing(train_data, max_len = MAX_TRAIN_LEN)\n",
        "x_test_ids, x_test_msk, x_test_typ, y_test_start, y_test_end = parsing(dev_data, max_len = MAX_TEST_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8I_vu-qP1nw"
      },
      "source": [
        "# vocabulary를 저장한다.\n",
        "with open('/data/vocabulary.pickle', 'wb') as f:\n",
        "    pickle.dump(tokenizer.get_vocab(), f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "# 학습 데이터를 저장한다.\n",
        "with open('/data/train_encoded.pickle', 'wb') as f:\n",
        "    pickle.dump([x_train_ids, x_train_msk, x_train_typ, y_train_start, y_train_end], f, pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "# 시험 데이터를 저장한다.\n",
        "with open('/data/test_encoded.pickle', 'wb') as f:\n",
        "    pickle.dump([x_test_ids, x_test_msk, x_test_typ, y_test_start, y_test_end], f, pickle.DEFAULT_PROTOCOL)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}